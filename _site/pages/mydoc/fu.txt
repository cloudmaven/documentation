

Video recordings of some of the presentations, including presentations of the final projects are available in 
[this YouTube channel](https://www.youtube.com/playlist?list=PLEdFhTRBFLObkatJOX9wp3BCueH4wNSl7)

On the second day of the workshop Cameron Craddock (Child Mind Institute, NY) presented a morning tutorial on using AWS for cloud computing 
for neuroimaging applications. The materials used for this tutorial are available on the 
[Neurohackweek GitHub account](https://github.com/neurohackweek/NI_cloud_computing) .

For this tutorial we provided participants with AWS credentials. They were thereby able to explore several options of neuroimaging compute 
pipelines with AWS, primarily built on EC2 and S3 technologies. 

## Project outcomes
Four of the team projects conducted during the week used AWS: 

1. Developing a system for running BIDS apps -- docker containers of neuroimaging pipelines -- in the cloud. 
Preprocessed data from the ABIDE initiative was used. When fully developed, this will greatly simplify user 
access to high powered neuroimaging pipelines available through BIDS-Apps and cloud computing. 

2. An implementation of the NPAIRS framework: Using a pre-configured AMI from Cameron, this project added 
nodes, installed python modules to the nodes, and ran machine learning scripts on the nodes. They found that it 
was easy to use a pre-built AMI but a little bit tricky to install modules to nodes. AWS or other cloud computing 
would be good for running time-consuming and parallel jobs that do not need lots of intermediate human interaction.

3. Collaborative Jupyter on AWS EC2: providing easy access to and storage of the ABIDE data and serial processing of 
scripts overnight. The scripts for this project were then run in parallel on AWS, making it easy to then deploy them 
on local clusters at the University of Oregon and at MIT when participants returned home. Notably, this team had 
some difficulty with credentials when trying to create new nodes. Quickly debugging these kinds of issues 
in the context of a hackathon is a challenge for the organizers.

4. Preprocess the [CoRR dataset](https://s3.amazonaws.com/fcp-indi/data/Projects/CORR): An AMI with the newest 
version (v0.4.0) of the C-PAC pipeline software was created, and Starcluster was used to launch and manage 
20 EC2 instances with the goal of preprocessing all of CoRR's datasets. Since CoRR  is so large (1629 subjects; 
3357 anatomical scans 5093 resting state functional scans), the instances didn't finish running before the end 
of Neurohackweek, but the setup during the week will allow the team to continue the work.


## Evaluation 

In addition, in a survey conducted on the last day of the workshop all participants were surveyed for their skills 
entering the workshop and for the learning that occurred during the workshop. These included, on cloud: 

How much knowledge about cloud computing did you have coming into Neurohackweek?

How much did you learn about cloud computing during Neurohackweek?

In free form responses, when asked "What kind of things did you learn at Neurohackweek?" several participants mentioned cloud 
computing. For example: 

"Learned rudimentary python and javascript, got a refresher on git/github, was introduced to many new tools like cloud computing."

"I learned how to use a bunch of tools I'd heard about but wasn't sure how to actually get working with my own data. I also 
worked with a great team to ask a scientific question using shared jupyter notebooks on the amazon cloud." 

On the other hand, several participants expressed frustration at not having enough time to learn more about these topics. 
When asked "What would you have liked to learn more about in Neurohackweek?", a few participants responded that more time 
should have been devoted to cloud computing. For example: 

"I couldn't get the cloud computing to work and I would very much have liked to learn more about it (at a more basic level... 
perhaps in a format like the github lecture... step by step so anyone can follow along and everyone has accomplished executing 
the mission-critical steps)."

"I wasn't able to absorb as much as I wanted about cloud computing. It was too much information for the limited time."

"If we'd had more time it would have been nice to have known what other folk are working on. I also need more time to understand 
the cloud computing/storage/docker implementations."

## Conclusion

Incorporating cloud computing in Neurohackweek was a ‘multiple win’ proposition. It facilitated projects by providing simple access 
for the participants to key neuroimaging computational tools. It helped identify bottlenecks to fix in cloud use. By training 
the participants Neurohackweek advanced the cultural shift in building and sharing open analytical tools in domain research. 
Finally of course Neurohackweek helped build awareness of the strengths of cloud computing. As noted we consider this an excellent 
starting point with strong continuation efforts to follow. We encourage other research communities to contact us about following 
the Neurohackweek model for community-building in other respective domains. 

abcdefghij




